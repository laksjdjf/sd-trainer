## EH
EHは私が考えた謎のネットワークです。Transformersの全結合層(to_q等)はトークンごとに埋め込みベクトルの全結合を行います。
EHでは埋め込みベクトルを $n$ グループに分けて全結合を行うことで、パラメータ数を $n^2$ 分の $1$ に抑えられます。

Original Linear:
$x_{out} = W_{org}x_{in}$

EH Linear:
$x_{out} = W_{org}x_{in} + \[W_{eh}x_{in}^{1} \cdots W_{eh}x_{in}^{n}\]^{T} \ \ \ (x_{in} = \[x_{in}^{1} \cdots  x_{in}^{n}\]^{T})$

$W_{out}$のサイズを( $r$ , $c$ )とするとき、 $W_{eh}$ のサイズは( $\frac{r}{n},\frac{c}{n}$ )となります。 $W_{out}$ をfrozenにして、 $W_{eh}$ のみ学習します。

$W_{eh}$の初期重みは0です。これにより学習の開始時点ではモデルの出力が同じになります。(LoRAやHypernetworksと同様)

ただの思い付きですが、そういえばmulti head attentionの分割数と同じにすればなんかすっきりするようなしないような気もしてきます。ただ実装が（特にv2では）めんどくさいのでやっていない。

## LoRAとの比較

優れているところ
+ これから見つけたい（笑）
+ down層による情報損失みたいなものがない、モジュールが1個で済むといった利点は思いついた。
+ 層の大きさによってパラメータが動的に変化する。LoRAの場合どんな次元もすべて同じrankにしてしまう（いや個別に設定すればいい話なんだが）。

同じところ
+ 元の重みにマージできる（はず）。
+ LoRA(EH)同士もマージできる（はず）。
+ マージに重み付けもできる（はず）。

劣っているところ
+ 一部のパラメータにしか作用しない
+ 手法の妥当性を示す論文がない＾＾
+ webui拡張などあらゆる便利な機能をこれ用に作る必要がある。
+ 特異値分解による抽出ができない
+ 開発者自身のスキル・知名度その他

とぅーどぅー
+ text encoderの学習に対応：自分がしないのでやりたくない
